#!/usr/bin/env python
# coding: utf-8

# # python pandas Basics:
# pandas is an open source library, providing high-performance, easy-to-use data structures and data analysis tools for Python.
# 
# The DataFrame is one of Pandas' most important data structures. It's basically a way to store tabular data where you can label the rows and the columns. One way to build a DataFrame is from a dictionary

# In[1]:


import pandas as pd

Putting data in a dictionary and then building a DataFrame works, but it's not very efficient. What if you're dealing with millions of observations? In those cases, the data is typically available as files with a regular structure. One of those file types is the CSV file,

to import CSV data into Python as a Pandas DataFrame you can use read_csv().
# Reading data url 

# In[2]:


df=pd.read_csv("https://raw.githubusercontent.com/akhil12028/Bank-Marketing-data-set-analysis/master/bank-additional-full.csv",sep=';')
df.head(5)


# In[3]:


df.tail(2)


# getting the datatype of the specific column in the dataframe

# In[4]:


df['age'].dtypes


# Rename the column name

# In[5]:


#rename will give new dataframe with changed name, the original dataframe column name remain same
df2=df.rename(columns={'job':'jobtype'},inplace=False)
df2
                


# In[6]:


df.isnull()
df.isnull().sum()


# In[7]:


#getting the random rows
df.sample()


# In[8]:


#getting column names 
df.columns


# In[9]:


#renaming columns
df3=pd.read_csv("https://raw.githubusercontent.com/akhil12028/Bank-Marketing-data-set-analysis/master/bank-additional-full.csv",sep=';')


# In[10]:


df3


# In[11]:


type(df3)


# Read Html file
# Pandas only reads the table from html file 

# In[12]:


df=pd.read_html("https://www.basketball-reference.com/leagues/NBA_2015_totals.html")
table1=df[0]
table1.head()


# In[13]:


table2=df[1]
#there is only one table


# In[ ]:


type(df)
#It reads the tables in the form of list, It contains tables as the list elements


# In[ ]:


table1['Player']
#reading one column


# In[ ]:


table1[['Player','Age']]


# List of labels. Note using [[]] returns a DataFrame.

# Methods to access the data frame
# loc : is label-based, which means that you have to specify rows and columns based on their row and column labels.
# 
# iloc : is integer index based, so you have to specify rows and columns by their integer index

# In[ ]:


table1.loc[[0,1]]


# Two Rows and one column

# In[ ]:


table1.loc[[0,1],'Player']


# In[ ]:


table1.loc[0:2,['Player','Age']]


# In[ ]:


#table1.loc[table1['Age']>20]
table1.dtypes


# In[ ]:


#iloc follow first included and last excluded indexing
#loc follows first and last included

table1.iloc[0:3]


# In[ ]:


table1.loc[0:3]


# In[ ]:


# Print sub-DataFrame
table1.loc[[0,1,2],['Player','Age']]


# In[ ]:


# Print out Age column as Series
print(table1['Age'])
type(table1['Age'])


# In[ ]:


# Print out Age column as DataFrame
print(table1[['Age']])
type(table1[['Age']])


# In[ ]:


# Print out Age, Player column as DataFrame
print(table1[['Age','Player']])


# Creating a Dataframe

# In[ ]:


df.dtypes


# Conditional return

# In[ ]:


df.loc[df['shield']>2]


# ndarray.ndim
# Number of array dimensions.

# In[ ]:


s = pd.Series({'a': 1, 'b': 2, 'c': 3})
s.ndim


# In[ ]:


df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})
df.ndim


# # using logical operators:
# np.logical_and()
# np.logical_or()
# np.logical_not()
# we use them on Pandas Series to do more advanced filtering operations

# In[ ]:


table3=pd.DataFrame({'Age':[20,25,19,18,40,21],'Rank':[1,2,3,4,5,6]},index=['Anu','Money','Danny','Kinni','Minni','Sachin'])
table3


# In[ ]:


import numpy as np
df3=table3[np.logical_and(table3['Age']>20,table3['Rank']>3)]
df3


# # Looping over dataframe:
# Iterating over a Pandas DataFrame is typically done with the iterrows() method. Used in a for loop, every observation( i.e row ) is iterated over and on every iteration the row label and actual row contents are available:
# 
# syntax:
# for lab, row in brics.iterrows() :
#     ...
# The row data that's generated by iterrows() on every run is a Pandas Series

# In[ ]:


# Iterate over rows 
for label,row_content in table3.head(2).iterrows():
    print(label+":")
    print(row_content)
    print("")
    print("type of lable:"+str(type(label)))
    print("type of content:"+str(type(row_content)))
    print("")


# # can use Data Frame attribute " .apply() " on dataframe by applying a function/Operation on another columns

# In[ ]:


# Use .apply(str.upper)
table3["Score"] =table3["Rank"].apply(table3['Rank']*2)    #creating a new column 
print(table3)


# In[ ]:


pd.options.display.max_rows=10
df=pd.read_csv('D:\Python_DataAnalytics\Linear Regression\Prediction',index_col=0)
df


# In[ ]:


df.head(10)


# # Load 10 records into the memory with nrows 
# Head Operation first load entire data into the memory and display 10 records by nrows load only 10 records into the memory.

# In[ ]:


df2=pd.read_csv('D:\Python_DataAnalytics\Linear Regression\Prediction',nrows=10)
df2


# In[ ]:


#It will give only 100 bytes of data & It doesnot return dataframe but give text file which can be read as a text file
df3=pd.read_csv('D:\Python_DataAnalytics\Linear Regression\Prediction',chunksize=100)
df3


# # Writing data to a text file
# to_csv is used to write a file

# In[ ]:


df2.to_csv("out.csv")
#It can write to excel, Jason and other formats
#it will store the index and column name as well


# In[ ]:


import sys
df2.to_csv("out.csv",index=False,header=False)


# In[ ]:


df2.to_csv("out3.csv",index=False,columns=['area','price'])


# # sys module
# sys â€” System-specific parameters and functions
# This module provides access to some variables used or maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.

# In[ ]:


df2.to_csv('out2.csv',sep='@',na_rep="No value")
#bydefault the seperator is ,na_rep is used to replace NAN value but its not used in practice. 


# # date_range give the range of dates
# data type is DatetimeIndex

# In[ ]:


date=pd.date_range('2/21/2021',periods=5)
date


# In[ ]:


date1=pd.date_range('2/21/2021',periods=5,freq='Y')
date1


# In[ ]:


type(date1)


# # numpy.arange is going to generate integer  
# and then convert Data into Series using pd.Series

# In[ ]:


import numpy as np
ts=pd.Series(np.arange(5))
ts


# In[ ]:


#give own indices
ts2=pd.Series(np.arange(5),index=date1)
ts2


# In[ ]:


#write down this time series data to a csv file
ts2.to_csv('tseries.csv',header=False)


# # reading a file using csv module

# In[ ]:


import csv
f=open('tseries.csv')
reader=csv.reader(f)
reader


# In[ ]:


for line in reader:
    print(line)


# In[ ]:


#Other Way to Read the Date
with open ('tseries.csv') as f:
    lines=list(csv.reader(f))
lines


# In[ ]:


header,values=lines[0],lines[1:]


# # Zip operation

# In[ ]:


list(zip(*values))


# In[ ]:


list(zip(header,zip(*values)))


# In[ ]:


#if no datatype is mentioned it returns tuple
x=1,2,3
x


# In[ ]:


y='a','b','c'
y


# In[ ]:


list(zip(x,y))


# # JSON File Read
# javascript object notation, Jason is a collection of dictionaries

# In[ ]:


obj="""{
	"id": "0001",
	"type": "donut",
	"name": "Cake",
	"ppu": 0.55,
	"batters":
		{
			"batter":
				[
					{ "id": "1001", "type": "Regular" },
					{ "id": "1002", "type": "Chocolate" },
					{ "id": "1003", "type": "Blueberry" },
					{ "id": "1004", "type": "Devil's Food" }
				]
		},
	"topping":
		[
			{ "id": "5001", "type": "None" },
			{ "id": "5002", "type": "Glazed" },
			{ "id": "5005", "type": "Sugar" },
			{ "id": "5007", "type": "Powdered Sugar" },
			{ "id": "5006", "type": "Chocolate with Sprinkles" },
			{ "id": "5003", "type": "Chocolate" },
			{ "id": "5004", "type": "Maple" }
		]
}"""


# In[15]:


import json
result=json.loads(obj)
result


# loads is used to convert string into dictionary
# Dumps convert it into string again

# In[ ]:


type(result)


# In[ ]:


string_result=json.dumps(result)
type(string_result)


# In[ ]:


df=pd.DataFrame(result['topping'])
df


# In[41]:


df2=pd.read_json('ProductJson.json',lines=True, orient=str)
df2


# In[45]:


df3=pd.read_json('ProductJson.json',lines=True, orient='records')
df3


# In[44]:


import json
with open('ProductJson.json', 'r') as datafile:
    data = json.load(datafile)
retail = pd.DataFrame(data)
retail


# In[17]:


obj="""{
"id": 2140,
"title": "gj",
"description": "ghj",
"location": "Hermannplatz 5-6, 10967 Berlin, Germany",
"lng": 0,
"lat": 0,
"userId": 4051,
"name": "manoj",
"isdeleted": false,
"profilePicture": "Images/9b291404-bc2e-4806-88c5-08d29e65a5ad.png",
"videoUrl": null,
"images": null,
"mediatype": 0,
"imagePaths": null,
"feedsComment": null,
"commentCount": 0,
"multiMedia": [
    {
"id": 3240,
"name": "",
"description": null,
"url": "http://www.youtube.com/embed/mPhboJR0Llc",
"mediatype": 2,
"likeCount": 0,
"place": null,
"createAt": "0001-01-01T00:00:00"
    }
  ],
"likeDislike": {
"likes": 0,
"dislikes": 0,
"userAction": 2
  },
"createdAt": "2020-01-02T13:32:16.7480006",
"code": 0,
"msg": null
}
"""


# In[20]:


result2=json.loads(obj)
result2


# In[24]:


result2['likeDislike']


# In[30]:


date=[1,2,3]
df=pd.DataFrame(result2['likeDislike'],index=date)
df

